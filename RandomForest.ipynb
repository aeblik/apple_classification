{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    ConfusionMatrixDisplay,\n",
    "    RocCurveDisplay,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score\n",
    ")\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import shap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest(features_file, output_folder):\n",
    "    # Ensure features_file and output_folder exist\n",
    "    if not os.path.exists(features_file):\n",
    "        raise FileNotFoundError(f\"File not found: {features_file}\")\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(features_file)\n",
    "    X = df.drop(columns=[\"Tree\", \"Apple\", \"Variety\", \"Augmented\", \"Mean_Color_0\", \"Mean_Color_1\", \"Mean_Color_2\", \"Std_Color_0\", \"Std_Color_1\", \"Std_Color_2\", \"Color_Range_0\", \"Color_Range_1\", \"Color_Range_2\"])\n",
    "    y = df[\"Variety\"]\n",
    "\n",
    "    # Split data\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.3, stratify=y_train_full, random_state=42\n",
    "    )\n",
    "\n",
    "    # Random Forest hyperparameter grid\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [50, 100, 200, 400, 600, 800],\n",
    "        \"max_depth\": [None, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    }\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model and parameters\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Validation Accuracy\n",
    "    val_accuracy = accuracy_score(y_val, best_rf.predict(X_val))\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Cross-Validation Scores\n",
    "    cross_val_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    cross_val_mean = np.mean(cross_val_scores)\n",
    "    cross_val_std = np.std(cross_val_scores)\n",
    "    print(f\"Cross-Validation Accuracy: Mean = {cross_val_mean:.4f}, Std = {cross_val_std:.4f}\")\n",
    "\n",
    "    # Test Metrics\n",
    "    y_test_pred = best_rf.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    # Print Test Metrics\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "\n",
    "    # Save results\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    results_file = f\"{output_folder}/random_forest_results.csv\"\n",
    "    results.to_csv(results_file, index=False)\n",
    "\n",
    "    # Generate plots\n",
    "    plot_feature_importance(best_rf, X.columns, output_folder)\n",
    "    plot_confusion_matrix(y_test, y_test_pred, output_folder)\n",
    "    plot_roc(\n",
    "    model=best_rf,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    class_names=y.unique(),\n",
    "    output_folder=output_folder,\n",
    "    )\n",
    "    plot_line_plot(results, output_folder, param=\"param_max_features\")\n",
    "    plot_n_estimators(results, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names, output_folder):\n",
    "    \"\"\"\n",
    "    Generate and save feature importance plot.\n",
    "    \"\"\"\n",
    "    feature_importances = model.feature_importances_\n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": feature_importances\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "    importance_df.to_csv(f\"{output_folder}/random_forest_feature_importance.csv\", index=False)\n",
    "\n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.barh(importance_df[\"Feature\"], importance_df[\"Importance\"])\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(\"Feature Importance (Random Forest)\")\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.ylabel(\"Features\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/random_forest_feature_importance.png\")\n",
    "    plt.close()\n",
    "\n",
    "def plot_confusion_matrix(y_test, y_test_pred, output_folder):\n",
    "    cm = confusion_matrix(y_test, y_test_pred)\n",
    "    plt.figure(figsize=(18, 14))\n",
    "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=np.unique(y_test), yticklabels=np.unique(y_test))\n",
    "    plt.title(\"RF Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.savefig(f\"{output_folder}/rf_confusion_matrix.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def plot_roc_curves(model, X_test, y_test, class_names, output_folder):\n",
    "    \"\"\"\n",
    "    Generate and save ROC curves plot.\n",
    "    \"\"\"\n",
    "    y_test_binarized = label_binarize(y_test, classes=class_names)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f\"Class {class_names[i]} (AUC = {roc_auc:.2f})\")\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=2)\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/roc_curves.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_roc(model, X_test, y_test, class_names, output_folder):\n",
    "    \"\"\"\n",
    "    Generate and save a single ROC curves plot for the top 5 and bottom 5 classes based on AUC.\n",
    "\n",
    "    Parameters:\n",
    "    - model: Trained RandomForestClassifier.\n",
    "    - X_test: Test set features.\n",
    "    - y_test: Test set true labels.\n",
    "    - class_names: List of class names.\n",
    "    - output_folder: Folder to save the plot.\n",
    "    \"\"\"\n",
    "    # Binarize test labels for ROC curve computation\n",
    "    y_test_binarized = label_binarize(y_test, classes=class_names)\n",
    "    y_pred_proba = model.predict_proba(X_test)\n",
    "    n_classes = len(class_names)\n",
    "\n",
    "    # Compute AUC for each class\n",
    "    roc_data = []\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = roc_curve(y_test_binarized[:, i], y_pred_proba[:, i])\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        roc_data.append((class_names[i], fpr, tpr, roc_auc))\n",
    "\n",
    "    # Sort classes by AUC\n",
    "    roc_data_sorted = sorted(roc_data, key=lambda x: x[3], reverse=True)\n",
    "    best_roc_data = roc_data_sorted[:5]  # Top 5 classes\n",
    "    worst_roc_data = roc_data_sorted[-5:]  # Bottom 5 classes\n",
    "\n",
    "    # Plot both top 5 and bottom 5 ROC curves\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Plot top 5 curves\n",
    "    for class_name, fpr, tpr, roc_auc in best_roc_data:\n",
    "        plt.plot(fpr, tpr, lw=2, label=f\"Top 5: {class_name} (AUC = {roc_auc:.2f})\")\n",
    "    \n",
    "    # Plot bottom 5 curves\n",
    "    for class_name, fpr, tpr, roc_auc in worst_roc_data:\n",
    "        plt.plot(fpr, tpr, lw=2, linestyle='--', label=f\"Worst 5: {class_name} (AUC = {roc_auc:.2f})\")\n",
    "    \n",
    "    # Plot diagonal for random classifier\n",
    "    plt.plot([0, 1], [0, 1], \"k--\", lw=2, label=\"Random Classifier\")\n",
    "\n",
    "    # Add plot details\n",
    "    plt.title(\"Top 5 and Bottom 5 ROC Curves\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.legend(loc=\"best\", fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/top_and_bottom_5_roc_curves.png\")\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "def plot_n_estimators(results, output_folder, metric=\"mean_test_score\"):\n",
    "    \"\"\"\n",
    "    Generate and save a plot showing the effect of n_estimators on the given metric.\n",
    "\n",
    "    Parameters:\n",
    "    - results: GridSearchCV results as a DataFrame.\n",
    "    - output_folder: Directory where the plot will be saved.\n",
    "    - metric: Metric to plot (default: \"mean_test_score\").\n",
    "    \"\"\"\n",
    "    # Filter results to focus only on n_estimators\n",
    "    n_estimators_results = results[[\"param_n_estimators\", metric]].copy()\n",
    "    n_estimators_results = n_estimators_results.groupby(\"param_n_estimators\")[metric].mean().reset_index()\n",
    "\n",
    "    # Plot using Matplotlib\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(n_estimators_results[\"param_n_estimators\"], n_estimators_results[metric], marker=\"o\", linestyle=\"-\", color=\"b\")\n",
    "    plt.title(f\"Effect of n_estimators on {metric}\")\n",
    "    plt.xlabel(\"n_estimators\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_folder}/n_estimators_{metric}.png\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 800}\n",
      "Validation Accuracy: 0.8947\n",
      "Cross-Validation Accuracy: Mean = 0.8534, Std = 0.0202\n",
      "Test Accuracy: 0.8798\n",
      "Test Precision: 0.8853\n",
      "Test Recall: 0.8798\n",
      "Test F1 Score: 0.8791\n"
     ]
    }
   ],
   "source": [
    "train_random_forest(\n",
    "        features_file=\"C:/Daten/PA2/Code/Output/apple_features_balanced_FE_Final_REVISED_1012.csv\",\n",
    "        output_folder=\"C:/Daten/PA2/Code/Output/ModelComparison_RF_1112_NoColor_n800\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results without Color:\n",
    "\n",
    "Best Parameters: {'max_depth': 20, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
    "\n",
    "Validation Accuracy: 0.8903\n",
    "\n",
    "Cross-Validation Accuracy: Mean = 0.8466, Std = 0.0251\n",
    "\n",
    "Test Accuracy: 0.8737\n",
    "\n",
    "Test Precision: 0.8795\n",
    "\n",
    "Test Recall: 0.8737\n",
    "\n",
    "Test F1 Score: 0.8732"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_random_forest_color(features_file, output_folder):\n",
    "    # Ensure features_file and output_folder exist\n",
    "    if not os.path.exists(features_file):\n",
    "        raise FileNotFoundError(f\"File not found: {features_file}\")\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Load data\n",
    "    df = pd.read_csv(features_file)\n",
    "    X = df.drop(columns=[\"Tree\", \"Apple\", \"Variety\", \"Augmented\"])\n",
    "    y = df[\"Variety\"]\n",
    "\n",
    "    # Split data\n",
    "    X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "        X, y, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_full, y_train_full, test_size=0.3, stratify=y_train_full, random_state=42\n",
    "    )\n",
    "\n",
    "    # Random Forest hyperparameter grid\n",
    "    param_grid = {\n",
    "        \"n_estimators\": [50, 100, 200, 400, 600, 800],\n",
    "        \"max_depth\": [None, 10, 20],\n",
    "        \"min_samples_split\": [2, 5, 10],\n",
    "        \"min_samples_leaf\": [1, 2, 4],\n",
    "        \"max_features\": [\"sqrt\", \"log2\", None],\n",
    "    }\n",
    "\n",
    "    # GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        RandomForestClassifier(random_state=42),\n",
    "        param_grid,\n",
    "        cv=5,\n",
    "        scoring=\"accuracy\",\n",
    "        n_jobs=-1,\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model and parameters\n",
    "    best_rf = grid_search.best_estimator_\n",
    "    best_params = grid_search.best_params_\n",
    "    print(f\"Best Parameters: {best_params}\")\n",
    "\n",
    "    # Validation Accuracy\n",
    "    val_accuracy = accuracy_score(y_val, best_rf.predict(X_val))\n",
    "    print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    # Cross-Validation Scores\n",
    "    cross_val_scores = cross_val_score(best_rf, X_train, y_train, cv=5, scoring=\"accuracy\")\n",
    "    cross_val_mean = np.mean(cross_val_scores)\n",
    "    cross_val_std = np.std(cross_val_scores)\n",
    "    print(f\"Cross-Validation Accuracy: Mean = {cross_val_mean:.4f}, Std = {cross_val_std:.4f}\")\n",
    "\n",
    "    # Test Metrics\n",
    "    y_test_pred = best_rf.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "    precision = precision_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    recall = recall_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    f1 = f1_score(y_test, y_test_pred, average=\"weighted\")\n",
    "    classification_report_str = classification_report(y_test, y_test_pred, output_dict=False)\n",
    "\n",
    "    # Print Test Metrics\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "    print(f\"Test Precision: {precision:.4f}\")\n",
    "    print(f\"Test Recall: {recall:.4f}\")\n",
    "    print(f\"Test F1 Score: {f1:.4f}\")\n",
    "    print(\"\\nClassification Report:\\n\", classification_report_str)\n",
    "\n",
    "    # Save results\n",
    "    results = pd.DataFrame(grid_search.cv_results_)\n",
    "    results_file = f\"{output_folder}/random_forest_results.csv\"\n",
    "    results.to_csv(results_file, index=False)\n",
    "\n",
    "    # Generate plots\n",
    "    plot_feature_importance(best_rf, X.columns, output_folder)\n",
    "    plot_confusion_matrix(y_test, y_test_pred, output_folder)\n",
    "    plot_roc_curves_top_5(\n",
    "    model=best_rf,\n",
    "    X_test=X_test,\n",
    "    y_test=y_test,\n",
    "    class_names=y.unique(),\n",
    "    output_folder=output_folder,\n",
    "    param_label=best_params\n",
    "    )\n",
    "    plot_line_plot(results, output_folder, param=\"param_max_features\")\n",
    "    plot_n_estimators(results, output_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
      "Validation Accuracy: 0.9481\n",
      "Cross-Validation Accuracy: Mean = 0.9604, Std = 0.0106\n",
      "Test Accuracy: 0.9556\n",
      "Test Precision: 0.9573\n",
      "Test Recall: 0.9556\n",
      "Test F1 Score: 0.9552\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      " 14-001-1296       1.00      0.83      0.91        30\n",
      " 14-001-1300       1.00      1.00      1.00        30\n",
      " 14-001-1353       0.91      0.97      0.94        30\n",
      " 14-001-1496       1.00      1.00      1.00        30\n",
      " 14-001-1504       0.88      0.73      0.80        30\n",
      " 14-001-1516       0.97      0.97      0.97        30\n",
      " 14-001-1529       1.00      1.00      1.00        30\n",
      " 14-001-1572       0.92      0.80      0.86        30\n",
      " 14-001-1579       1.00      1.00      1.00        30\n",
      " 14-001-1873       0.93      0.87      0.90        30\n",
      " 14-001-1878       0.97      0.93      0.95        30\n",
      " 14-001-1953       1.00      1.00      1.00        30\n",
      " 14-001-2042       0.97      1.00      0.98        30\n",
      " 14-001-2103       0.88      0.97      0.92        30\n",
      " 14-001-2266       1.00      0.97      0.98        30\n",
      " 14-001-2526       0.97      1.00      0.98        30\n",
      " 14-001-2551       1.00      1.00      1.00        30\n",
      " 14-001-2640       1.00      1.00      1.00        30\n",
      " 14-001-2651       1.00      1.00      1.00        30\n",
      " 14-001-2748       1.00      1.00      1.00        30\n",
      " 14-001-2791       1.00      1.00      1.00        30\n",
      " 14-001-2860       0.94      1.00      0.97        30\n",
      " 14-001-3019       0.93      0.93      0.93        30\n",
      " 14-001-3232       0.94      0.97      0.95        30\n",
      " 14-001-3234       0.79      0.90      0.84        30\n",
      " 14-001-3239       1.00      1.00      1.00        30\n",
      " 14-001-3240       0.96      0.90      0.93        30\n",
      " 14-001-3244       0.94      1.00      0.97        30\n",
      " 14-001-3245       0.81      0.97      0.88        30\n",
      " 14-001-3247       0.97      1.00      0.98        30\n",
      "  14-001-358       1.00      1.00      1.00        30\n",
      "  14-001-458       0.96      0.87      0.91        30\n",
      "  14-001-846       0.97      0.97      0.97        30\n",
      "\n",
      "    accuracy                           0.96       990\n",
      "   macro avg       0.96      0.96      0.96       990\n",
      "weighted avg       0.96      0.96      0.96       990\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_random_forest_color(\n",
    "        features_file=\"C:/Daten/PA2/Code/Output/apple_features_balanced_FE_Final_REVISED_1012.csv\",\n",
    "        output_folder=\"C:/Daten/PA2/Code/Output/ModelComparison_RF_1112_WithColor_n800\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results with Color:\n",
    "\n",
    "Best Parameters: {'max_depth': 20, 'max_features': 'sqrt', 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 400}\n",
    "Validation Accuracy: 0.9637\n",
    "\n",
    "Cross-Validation Accuracy: Mean = 0.9383, Std = 0.0140\n",
    "\n",
    "Test Accuracy: 0.9546\n",
    "\n",
    "Test Precision: 0.9586\n",
    "\n",
    "Test Recall: 0.9546\n",
    "\n",
    "Test F1 Score: 0.9539"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "My Env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
